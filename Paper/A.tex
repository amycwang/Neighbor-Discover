\section{Partially-Connected Networks}
\label{PCN}



%background and instances
A practical scenario is that in a network,
all the nodes are partially connected with each other.
A node can discover a fraction of nodes within its sensing 
range.

%density function and expectation of the neighbor number
In a partially-connected network for $\forall$ node $u_i$, 
its position coordinate $(x_i,y_i)$
obeys a certain probability distribution depending on the 
characteristic of the network, with the density function as :
$$f(x,y)=
\begin{cases}
\varphi(x,y)& (x,y)\in D\\
0& (x,y)\notin D
\end{cases}$$
where $D$ is the network covering area.

$\forall$ node $u_i (x_i,y_i)$, its sensing range area $R_i$ can be formulated as:
$$
(x-x_i)^2+(y-y_i)^2 \leq r^2
$$
where $r$ is the detection radius.

Thus, we can obtain the expected number of neighbors of node $u_i$ as :
$$
NB(u_i) = N\iint_{R_i} f(x,y)\,dx\,dy - 1.
$$

We ignore the boundary area of the network and assume the
nodes in the network is of an enormous quantity, so the 
expectation neighbors can be formulated as:
$$
NB(u_i) = N\iint_{R_i} \varphi(x,y)\,dx\,dy.
$$

Note that, when the network area is far more larger than the
sensing area of the nodes, we can approximately get:
$$
NB(u_i) = N\pi r^2 \varphi(x,y).
$$




%Alano
Then we propose \textbf{Alano}, a randomized neighbor discovery algorithm. 
We describe the algorithm for $\forall$ node $u_i$ in Alg. \ref{Alano}.
Alano algorithm indicates that what probability for a node choose to turn to  
tramisitting state or listening state is determined
by the expectation neighbor number varying from node to node.


\begin{algorithm}
\caption{Alano Algorithm}
\label{Alano}
\begin{algorithmic}[1]
\STATE $\hat{n_i} = N\iint_{R_i} \varphi(x,y)\,dx\,dy$;
\STATE $p_t^i = \frac{1}{\hat{n_i}}$;
\WHILE {$True$}
	\STATE A random float $\epsilon \in (0,1)$;
   	 \IF{$\epsilon < p_t$}
    		\STATE Transmit a message containing node information of $u_i$;
	\ELSE
    		\STATE Listen on the channel and decode the node information if receive a message successfully;
	\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In the following section \ref{uniform}, we first consider a general situation that the nodes
in the network are uniform distributed. We derive a proof that the probability
chosen in Alano is the optimal one and show the bounded latency will not 
be much larger than its expectation. Then in the section  \ref{normal} we describe a more
common situation that the nodes in the network obey Gaussian Distribution, we present a 
approximation analysis that the discovery latency will not be much larger than uniform distribution.


\subsection{Uniform Distribution}
\label{uniform}
%background
There are a part of partially-connected networks obeying uniform distribution. 
For instance, consider there is a  wireless sensor network carrying out a task of
measuring temperature and humidity in a target area, 
thus the sensors are supposed to be evenly deployed and the density function can be 
formulated as:
$$f(x)=
\begin{cases}
\frac{1}{A}& (x,y)\in D\\
0& (x,y)\notin D
\end{cases}$$
where $A$ is the area of $D$.

Every node in the network has the same expectation 
of neighbor number and transmit with the same probability as:
$$
\hat{n} = \frac{N\pi r^2}{A}, \quad p_t = \frac{1}{\hat{n}}=\frac{A}{N\pi r^2}.
$$  

According to Alano, the probability that node $u_i$ discover a specific
neighbor successfully in a time slot can be formulated as:
$$
p_s = p_t{(1-p_t)}^{\hat{n}-1}.
$$
Let:
$$
p_s' = {(1-p_t)}^{\hat{n}-1}-(\hat{n}-1)p_t{(1-p_t)}^{\hat{n}-2}=0.
$$
It is easy to confirm that  when
$$p_t=\frac{1}{\hat{n}}.$$ 
$p_s$ gets the maximum value:
$$p_s = \frac{1}{\hat{n}}{(1-\frac{1}{\hat{n}})}^{\hat{n}-1} \approx \frac{1}{\hat{n}e}.$$

Thus we can conclude that the probability chosen in Alano
to transmit is the optimal one. 

Next we analyse the expectation latency for a node to discover all its 
neighbors. We denote $W_j$ to be a random variable representing the number of the time slots 
needed to discover a new neighbor after $(j-1)$ neighbors have been discovered, which follows 
Geometric distribution with parameter $p(j): p(j)=(\hat{n}-j+1)p_s$. Then the expectation
of $W_j$ is computed as:
$$
E[W_j]=\frac{1}{p(j)}=\frac{1}{(\hat{n}-j+1)p_s}
$$

The expectation time latency of discovering all the neighbors can be formulated as:
$$
E[W_j] = \sum_{j=1}^{\hat{n}}\frac{1}{p_s}H_n \approx ne(lnn + \Theta(1)) = \Theta(nlnn).
$$
where $H_n$ is the $n$-th Harmonic number, i.e.,
$H_n = lnn + \Theta(1)$.

We get the expectation discovery latency is within O(nlogn) and then we
show the bounded latency will not be much larger than its expectation.

%To be modified later
%%
If $W_i$ is given, the value of $W_j$ will not be affected for $i<j$. That is, for $i\ne j$, $W_i$ and $W_j$ are independent and they satisfy $P(W_j=w_j|W_i=w_i)=P(W_j=w_j)$. Since $W_j$ follows Geometric distribution, and $Var[W_j]=\frac{1-p_j}{p_j^2}$, the variance of $W$ is
\begin{displaymath}
\begin{split}
 Var[W] %& =Var[\sum_{j=1}^{p_nN}W_j]=\sum_{j=1}^{p_nN}Var[W_j]+\sum_{j\ne k}Cov[W_j,W_k] \\
 =\sum_{j=1}^{n}Var[W_j]
%=\frac{1}{p_{suc}^2}\sum_{j=1}^{p_nN}\frac{1}{j^2}-\frac{1}{p_{suc}}\sum_{j=1}^{p_nN}\frac{1}{j} \\
 \le\frac{\pi^2}{6p_{suc}^2}-\frac{H_n}{p_{suc}}.
\end{split}
\end{displaymath}

With \emph{Chebyshev's inequality}, the probability that the discovery time is 2 times larger than the expectation is
\begin{displaymath}
\begin{split}
P[W\ge2E[W]]%=P[|W-E[W]|\ge E[W]]
\le\frac{Var[W]}{E[W]^2}
%&\le\frac{\frac{\pi^2}{6p_{suc}^2}-\frac{H_{p_nN}}{p_{suc}}}{\frac{H_{p_nN}^2}{p_{suc}^2}}=
\le\frac{\pi^2}{6H_{n}^2}-\frac{p_{suc}}{H_n}.
\end{split}
\end{displaymath}
For large $n$, $P[W\ge2E[W]]$ is close to 0. That is, the time for a node to find all neighbors is very likely to be smaller than $2$ times of expected latency. Therefore,
\begin{equation*}
W=O(nlnn).
\end{equation*}

%%



%*****************








\subsection{Gaussian Distribution}
\label{normal}
%background
A common scenario is that the nodes in a network obey 2D Gaussian distribution.
For example, an intrusion detection application may need improved detection 
probability around important entities\cite{wang2013gaussian}.

In this section, we present a theoretical proof that the latency performance
will not be much larger than uniform distribution.
The analysis is not only applicable for Gaussian distribution
but also flexible for all the other distributions.

We first denote the approximate neighbors of node $u_i$
as set $S(u_i) = \{u_{i1},u_{i2},...,u_{i\hat{n_i}}\}$.
When the nodes obey Gaussian distribution, according to Alano, 
the probability that node $u_i$ discovers a certain 
neighbor node $u_{ij}$ successfully in a time slot can be formulated as:
$$
p_{suc} = (1-p_t^i)p_t^{ij}\prod_{ k=1}^{\hat{n_i}, k\neq j}(1-p_t^{ik})
$$

Denote:
$$
p_t^{imax} = \max_{1 \leq j \leq \hat{n_i}}\{p_t^{ij}\}, \quad p_t^{imin} = \min_{1 \leq j \leq \hat{n_i}}\{p_t^{ij}\}.
$$

Thus for $\forall j$, $1 \leq j \leq n_i$:
\begin{align*}
&(1-p_t^i)p_t^{imin}{(1-p_t^{imax})}^{\hat{n_i}-1} \\
\leq &(1-p_t^i)p_t^{ij}\prod_{ k=1}^{\hat{n_i}, k\neq j}(1-p_t^{ik}) \\
\leq &(1-p_t^i)p_t^{imax}{(1-p_t^{imin})}^{\hat{n_i}-1} 
\end{align*}

Denote:
\begin{align*}
&P = (1-p_t^i)p_t^{imin}{(1-p_t^{imax})}^{\hat{n_i}-1}  \\
&Q = (1-p_t^i)p_t^{imax}{(1-p_t^{imin})}^{\hat{n_i}-1} 
\end{align*}

Thus we get:
\begin{align*}
\frac{1}{\hat{n_i}Q} \quad \leq \quad &E[W_1] \quad \leq \quad \frac{1}{\hat{n_i}P} \\
\frac{1}{(\hat{n_i}-1)Q} \quad \leq \quad &E[W_2] \quad \leq \quad \frac{1}{(\hat{n_i}-1)P} \\
& ..........\\
\frac{1}{Q} \quad \leq \quad &E[W_{\hat{n_i}}] \quad \leq \quad \frac{1}{P} 
\end{align*}

Sum all the equations above, and we will get:
$$
\frac{1}{Q}H_n  \quad \leq \quad E[\sum_{j=1}^{\hat{n_i}}W_j]  \quad \leq \quad \frac{1}{P}H_n
$$

Since the sensible neighbors are within a close distance of the node 
compared to the total network area, 
which implies the density function values are within the same order of magnitude.
Thus we can conclude that the expectation of the time latency in the normal 
distributed networks are still $E[W]=O(nlnn)$.
Similarly the bounded latency can be proved to be $W=O(nlnn)$ in the same way as
uniform distribution.


