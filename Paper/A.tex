\section{Large-Scale Networks}
\label{PCN}

%Expectation	#meester1996continuum

%background and instances
In a large-scale network and the unit-disk model, 
all the nodes are partially-connected with each other.
A node can discover the nodes within its sensing 
range.

%density function and expectation of the neighbor number
In the network for $\forall$ nodes $u_i$, 
it is possible for the nodes to be aware of its position coordinates $(x_i,y_i)$ 
and get the local density by using a pre-defined density function based on 
the deployment of the network application.

The expected number of neighbors can be computed as 
follows \cite{meester1996continuum, wang2015connectivity}.

Denote the density function as :
$$f(x,y)=
\begin{cases}
\varphi(x,y)& (x,y)\in D\\
0& (x,y)\notin D
\end{cases}$$
where $D$ is the network covering area.

$\forall$ node $u_i (x_i,y_i)$, its sensing range area $R_i$ can be formulated as:
$$
(x-x_i)^2+(y-y_i)^2 \leq r^2
$$
where $r$ is the detection radius.

Thus, we can obtain the expected number of neighbors of node $u_i$ as :
$$
NB(u_i) = N\iint_{R_i} f(x,y)\,dx\,dy - 1.
$$

In the large-scale networks we ignore the boundary area of the network and assume the
nodes in the network is of an expansive quantity, so the 
expected number of neighbors can be formulated as follows:
$$
NB(u_i) = N\iint_{R_i} \varphi(x,y)\,dx\,dy.
$$

Note that, when the network area is far more larger than the
sensing area of the nodes, we approximate it as follows:
$$
NB(u_i) = N\pi r^2 \varphi(x,y).
$$




%Alano
Here we propose \textbf{Alano}, a randomized neighbor discovery algorithm. 
We describe the algorithm for $\forall$ node $u_i$ in the algorithm. \ref{Alano}.
The Alano algorithm defines at what probability a node chooses to be in a  
transmitting state or a listening state. The probablity function is determined
from the expected number of neighbors varying from node to node.


\begin{algorithm}
\caption{Alano Algorithm}
\label{Alano}
\begin{algorithmic}[1]
\STATE $\hat{n_i} = N\iint_{R_i} \varphi(x,y)\,dx\,dy$;
\STATE $p_t^i = \frac{1}{\hat{n_i}}$;
\WHILE {$True$}
	\STATE A random float $\epsilon \in (0,1)$;
   	 \IF{$\epsilon < p_t$}
    		\STATE Transmit a message containing node information of $u_i$;
	\ELSE
    		\STATE Listen on the channel and decode the node information if receive a message successfully;
	\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In the following section \ref{uniform}, we consider a general situation that uniformly distributes nodes
in the network. We then derive a proof that the probability
chosen in Alano is the optimal one and show the bounded latency will not 
be larger than its expectated. Then in the section  \ref{normal} we have described a more
common situation that the nodes in the network obey Gaussian Distribution, we have presented a 
approximation analysis that shows the discovery latency will not be much larger than in uniform distribution.


\subsection{Uniform Distribution}
\label{uniform}
%background
Uniform distribution is a basic component in the deployment of wireless networks. 
For instance, consider a  wireless sensor network
measuring temperature and humidity in a target area, 
it is essential the sensors are evenly deployed and the density function can be 
formulated as:
$$f(x)=
\begin{cases}
\frac{1}{A}& (x,y)\in D\\
0& (x,y)\notin D
\end{cases}$$
where $A$ is the area of $D$.

Every node in the network has the same expected number of 
neighbors and transmit with the same probability, which is given as:
$$
\hat{n} = \frac{N\pi r^2}{A}, \quad p_t = \frac{1}{\hat{n}}=\frac{A}{N\pi r^2}.
$$  

As defined by Alano, the probability that node $u_i$ discover a specific
neighbor successfully in a time slot can be formulated as:
$$
p_s = p_t{(1-p_t)}^{\hat{n}-1}.
$$
Let:
$$
p_s' = {(1-p_t)}^{\hat{n}-1}-(\hat{n}-1)p_t{(1-p_t)}^{\hat{n}-2}=0.
$$
It is easy to confirm that when
$$p_t=\frac{1}{\hat{n}}.$$ 
$p_s$ gets the maximum value:
$$p_s = \frac{1}{\hat{n}}{(1-\frac{1}{\hat{n}})}^{\hat{n}-1} \approx \frac{1}{\hat{n}e}.$$

Thus we can conclude that the probability chosen in Alano
to transmit is the optimal one. 

Next we analyze the expected latency for a node to discover all its 
neighbors. We denote $W_j$ to be a random variable representing the number of the time slots 
needed to discover a new neighbor after $(j-1)$ neighbors have been discovered, which follows 
Geometric distribution with parameter $p(j): p(j)=(\hat{n}-j+1)p_s$. Then the expected $W_j$ is computed as:
$$
E[W_j]=\frac{1}{p(j)}=\frac{1}{(\hat{n}-j+1)p_s}
$$

The expectation time latency of discovering all the neighbors can be formulated as:
$$
E[W_j] = \sum_{j=1}^{\hat{n}}\frac{1}{p_s}H_n \approx ne(lnn + \Theta(1)) = \Theta(nlnn).
$$
where $H_n$ is the $n$-th Harmonic number, i.e.,
$H_n = lnn + \Theta(1)$.

We obtain the expectated discovery latency within O(n logn) and then we
show the bounded latency will not be much larger than expected.

%To be modified later
%%
If $W_i$ is given, the value of $W_j$ will not be affected for $i<j$. That is, for $i\ne j$, $W_i$ and $W_j$ are independent and they satisfy $P(W_j=w_j|W_i=w_i)=P(W_j=w_j)$. Since $W_j$ follows Geometric distribution, and $Var[W_j]=\frac{1-p_j}{p_j^2}$, the variance of $W$ is
\begin{displaymath}
\begin{split}
 Var[W] %& =Var[\sum_{j=1}^{p_nN}W_j]=\sum_{j=1}^{p_nN}Var[W_j]+\sum_{j\ne k}Cov[W_j,W_k] \\
 =\sum_{j=1}^{n}Var[W_j]
%=\frac{1}{p_{suc}^2}\sum_{j=1}^{p_nN}\frac{1}{j^2}-\frac{1}{p_{suc}}\sum_{j=1}^{p_nN}\frac{1}{j} \\
 \le\frac{\pi^2}{6p_{suc}^2}-\frac{H_n}{p_{suc}}.
\end{split}
\end{displaymath}

With \emph{Chebyshev's inequality}, the probability that the discovery time is 2 times larger than expectated is
\begin{displaymath}
\begin{split}
P[W\ge2E[W]]%=P[|W-E[W]|\ge E[W]]
\le\frac{Var[W]}{E[W]^2}
%&\le\frac{\frac{\pi^2}{6p_{suc}^2}-\frac{H_{p_nN}}{p_{suc}}}{\frac{H_{p_nN}^2}{p_{suc}^2}}=
\le\frac{\pi^2}{6H_{n}^2}-\frac{p_{suc}}{H_n}.
\end{split}
\end{displaymath}
For large $n$, $P[W\ge2E[W]]$ is close to 0. That is, the time for a node to find all neighbors is very likely to be smaller than $2$ times of expected latency. Therefore,
\begin{equation*}
W=O(nlnn).
\end{equation*}

%%



%*****************








\subsection{Gaussian Distribution}
\label{normal}
%background
A common scenario is that the nodes in a network obey 2D Gaussian distribution.
For example, an intrusion detection application may need improved detection 
probability around important entities\cite{wang2013gaussian}.

In this section, we present a theoretical proof that the latency performance
will not be much larger than uniform distribution.
The analysis is not only applicable for Gaussian distribution
but also flexible for all the other distributions in a large-scale network.

We first denote the approximate neighbors of node $u_i$
as set $S(u_i) = \{u_{i1},u_{i2},...,u_{i\hat{n_i}}\}$.
When the nodes obey Gaussian distribution, according to Alano, 
the probability that node $u_i$ discovers a certain 
neighbor node $u_{ij}$ successfully in a time slot can be formulated as:
$$
p_{suc} = (1-p_t^i)p_t^{ij}\prod_{ k=1}^{\hat{n_i}, k\neq j}(1-p_t^{ik})
$$

Denote:
$$
p_t^{imax} = \max_{1 \leq j \leq \hat{n_i}}\{p_t^{ij}\}, \quad p_t^{imin} = \min_{1 \leq j \leq \hat{n_i}}\{p_t^{ij}\}.
$$

Thus for $\forall j$, $1 \leq j \leq n_i$:
\begin{align*}
&(1-p_t^i)p_t^{imin}{(1-p_t^{imax})}^{\hat{n_i}-1} \\
\leq &(1-p_t^i)p_t^{ij}\prod_{ k=1}^{\hat{n_i}, k\neq j}(1-p_t^{ik}) \\
\leq &(1-p_t^i)p_t^{imax}{(1-p_t^{imin})}^{\hat{n_i}-1} 
\end{align*}

Denote:
\begin{align*}
&P = (1-p_t^i)p_t^{imin}{(1-p_t^{imax})}^{\hat{n_i}-1}  \\
&Q = (1-p_t^i)p_t^{imax}{(1-p_t^{imin})}^{\hat{n_i}-1} 
\end{align*}

Thus we get:
\begin{align*}
\frac{1}{\hat{n_i}Q} \quad \leq \quad &E[W_1] \quad \leq \quad \frac{1}{\hat{n_i}P} \\
\frac{1}{(\hat{n_i}-1)Q} \quad \leq \quad &E[W_2] \quad \leq \quad \frac{1}{(\hat{n_i}-1)P} \\
& ..........\\
\frac{1}{Q} \quad \leq \quad &E[W_{\hat{n_i}}] \quad \leq \quad \frac{1}{P} 
\end{align*}

Sum all the equations above, and we will get:
$$
\frac{1}{Q}H_n  \quad \leq \quad E[\sum_{j=1}^{\hat{n_i}}W_j]  \quad \leq \quad \frac{1}{P}H_n
$$


%CORRECT THIS!!!!!
Since the sensible neighbors are within a close distance of the node 
compared to the total network area, 
which implies the density function values are within the same order of magnitude.
Thus we can conclude that the expectation of the time latency in the normal 
distributed networks are still $E[W]=O(nlnn)$.
Similarly the bounded latency can be proved to be $W=O(nlnn)$ in the same way as
uniform distribution.


